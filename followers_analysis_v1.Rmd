---
title: "followers_analysis_dim_reduction"
author: "Javier Fernandez"
date: "5/1/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(knitr)
library(scales)
library(tidyverse)
library(here)
library(dplyr)
library(patchwork)
library(ggrepel)
library(directlabels)
library(ggbeeswarm)
library(ggjoy)
library(ggExtra)
library(funModeling)
library(purrr)
library(skimr)
library(sentimentr)
library(stringr) 

```

## Loading data

```{r}
data <- read_csv(here('data','playlists_data.csv'))
#skim(data)
```

## Let's do a little exploration!

```{r}

data %>% 
  filter(owner=='spotify') %>% 
  ggplot(aes(monthly_owner_stream30s)) +
  geom_histogram()

data %>% 
  count(genre_1, 
        sort=TRUE)
data %>% 
  count(mood_1
        , sort=TRUE)



data %>% 
rowwise() %>%
  mutate(number_genres = sum(c_across(genre_1:genre_3) != "-"),
         number_moods =  sum(c_across(mood_1:mood_3) != "-")) %>%
  ungroup() %>%
  select(genre_1:genre_3, number_genres, mood_1:mood_3, number_moods ) %>%
  count(number_genres, sort = TRUE)

# Sentiment analysis
sentences <-  get_sentences(gsub('\\d|\U{FFFFFFFF}|\U3e33613c|\U3e63613c|\\[|\\]|\u00f1a', '',  enc2utf8(data$tokens)))
sentiments <- sentiment_by(sentences, list(data$playlist_uri, enc2utf8(data$tokens))) 

sentiments_idx <- sentiments %>% 
  rename(tokens_count = word_count) %>%
  select(-`tokens)`, -sd)

data_with_sentiment_score <-
inner_join(sentiments_idx, data, by='playlist_uri')


data_with_sentiment_score %>% 
  rowwise() %>%
  mutate(number_genres = sum(c_across(genre_1:genre_3) != "-"),
       number_moods =  sum(c_across(mood_1:mood_3) != "-")) %>%
  ungroup() 
  


  
```


```{r}
data %>% 
  mutate(monthly_stream30_followers=monthly_stream30s-monthly_owner_stream30s,
         mau_engagement=replace_na(mau_both_months*100/mau_previous_month,0),
         ratio_durable_streamings=replace_na(stream30s*100/streams,0)
         ) %>%
  filter(mau>3) %>%
  select(monthly_stream30_followers) %>% 
  ggplot(aes(x = factor(1), y = log10(monthly_stream30_followers)))+
  geom_boxplot(width = 0.4, fill = "white") +
  geom_jitter(width = 0.1, size = 1)
```


#Feature Engineering: preparing the data
Let's work on Long Term Successes: MAU
```{r}
data_long_term <- data %>% 
  mutate(monthly_stream30_followers=monthly_stream30s-monthly_owner_stream30s,
         mau_engagement=replace_na(mau_both_months*100/mau_previous_month,0),
         owner_type = case_when(owner == 'spotify' ~ 'spotify',
                                TRUE ~ "created_by_user")
         ) %>%
  filter(mau>3,
         monthly_stream30_followers < 3*10^7) %>%
  rowwise() %>%
  mutate(number_genres = sum(c_across(genre_1:genre_3) != "-"),
       number_moods =  sum(c_across(mood_1:mood_3) != "-")) %>%
  ungroup() %>% 
  select(-genre_2,-genre_3,-mood_2, -mood_3, -monthly_stream30s, -monthly_owner_stream30s, n_local_tracks, -owner_country, -dau,-wau, -streams, -stream30s, -n_local_tracks)

sentences <-  get_sentences(gsub('\\d|\U{FFFFFFFF}|\U3e33613c|\U3e63613c|\\[|\\]|\u00f1a', '',  enc2utf8(data_long_term$tokens)))
sentiments <- sentiment_by(sentences, list(data_long_term$playlist_uri, enc2utf8(data_long_term$tokens))) 

sentiments_idx <- sentiments %>% 
  rename(tokens_count = word_count) %>%
  select(-`tokens)`, -sd)

data_cleaned <-
inner_join(sentiments_idx, data_long_term, by='playlist_uri') %>% 
  select(-tokens) %>% 
  mutate_if(is.character, factor)

#skim(data_cleaned)
  
  
```


```{r}
data_cleaned %>% 
 ggplot(aes(x=monthly_stream30_followers)) +
  aes(y=stat(count)/sum(stat(count))) + 
  geom_histogram(binwidth=50, boundary = 0, color="black", fill="#20D760")+
  scale_x_continuous(expand = expansion(mult = c(0,0)),
                     limits = c(-5,500), 
                     breaks = seq(0,500,50)
                     )+
  scale_y_continuous(labels = scales::percent)+
  labs(y = "Number of Playlists", 
       x="Monthly streams >30s of followers")+
  theme_minimal()+
  theme(
  axis.text.x = element_text(size = 8),
  axis.title.y = element_text(size = 10))

data_cleaned %>% 
  filter(monthly_stream30_followers>7000)%>%
  summarise(count_percent = (n()/102891)*100,
            n())

```



```{r}
library(embed)
library(recipes)

#data_super_cleaned <- data_cleaned %>% filter(monthly_stream30_followers>7000)

umap_rec <- recipe(~., data = data_cleaned) %>%
  update_role(playlist_uri, owner, new_role = "id") %>%
  step_corr(all_numeric()) %>%
  step_dummy(all_nominal(), -has_role("id")) %>%
  step_zv(all_numeric()) %>%
  step_normalize(all_predictors()) %>% 
  step_umap(all_predictors()) 

umap_prep <- prep(umap_rec)
  
umap_prep %>% juice()



bake(umap_prep, new_data = NULL) %>%
  ggplot(aes(umap_1, umap_2)) +
  geom_point(color = "midnightblue", alpha = 0.3, size = 0.5) +
 # geom_text(check_overlap = TRUE, hjust = "inward", family = "IBMPlexSans") +
  labs(color = NULL)
  


hdb <- bake(umap_prep, new_data = NULL) %>%
  select(umap_1, umap_2) %>% 
  hdbscan(minPts = 50)




# Modeling packages
library(mclust)   # for fitting clustering algorithms
# Apply GMM model with 3 components
data_mc <- Mclust(umap_prep %>% juice() %>% select(umap_1, umap_2), G = 7)

# Plot results
plot(data_mc, what = "density")
plot(data_mc, what = "uncertainty")

data_optimal_mc <- Mclust(umap_prep %>% juice() %>% select(umap_1, umap_2))
legend_args <- list(x = "bottomright", ncol = 5)
plot(data_optimal_mc, what = 'BIC', legendArgs = legend_args)
plot(data_optimal_mc, what = 'classification')
plot(data_optimal_mc, what = 'uncertainty')


data_optimal_mc %>% mutate(cluster=classification)

umap_prep %>% juice() %>% mutate(cluster=data_optimal_mc$classification,
                                 uncertainty=data_optimal_mc$uncertainty)



uncertainty <- data.frame(
  id = 1:nrow(umap_prep %>% juice()),
  cluster = data_optimal_mc$classification,
  uncertainty = data_optimal_mc$uncertainty
)

uncertainty %>%
  group_by(cluster) %>%
  filter(uncertainty > 0.25) %>%
  ggplot(aes(uncertainty, reorder(id, uncertainty))) +
  geom_point() +
  facet_wrap(~ cluster, scales = 'free_y', nrow = 1)


```



# Grid Search of parameters 
```{r}
n_neighbors <- c(15,30,50,100,150)
min_distance <- c( 0.001, 0.003, 0.009,0.03,0.09)
metrics <- c("euclidean" ,"cosine","hamming")

umap_rec <- recipe(~., data = data_cleaned) %>%
  update_role(playlist_uri, owner, new_role = "id") %>%
  step_corr(all_numeric()) %>%
  step_dummy(all_nominal(), -has_role("id")) %>%
  step_zv(all_numeric()) %>%
  step_normalize(all_predictors()) 
  




data_embs <- data_cleaned #copy of the data
for (nn in n_neighbors) {
 for (md in min_distance) {
  for (metr in metrics) {
    umap_prep <- umap_rec %>% step_umap(all_predictors(),
                           neighbors = nn,
                           min_dist = md,
                           options = list(metric = metr,
                                          verbose = TRUE, 
                                          n_threads = 1)) %>% prep()
     umap_embedding <- umap_prep %>% juice() 
  data_embs <- data_embs %>% 
  bind_cols(umap_embedding['umap_1'] %>% as_tibble() ) %>% 
  bind_cols(umap_embedding['umap_2'] %>% as_tibble() ) 
  names(data_embs)[names(data_embs) == 'umap_1' ] = paste('nn_',nn,'md_',md,'metric',metr,'1',sep = '.')
  names(data_embs)[names(data_embs) == 'umap_2' ] = paste('nn_',nn,'md_',md,'metric',metr,'2',sep = '.')
  }
 }
 
}



```


#Saving File

```{r}
write.csv(data_embs,here('data','playlists_embeds_trials.csv'), row.names = FALSE)
```


#Visualizing first example

```{r}

n_neighbors <- c(15,30,50,100,150)
min_distance <- c( 0.001, 0.003, 0.009,0.03,0.09)
metrics <- c("euclidean" ,"cosine","hamming")
for (nn in n_neighbors) {
 for (md in min_distance) {
  for (metr in metrics) {
data_embs %>%
 ggplot(aes(x = data_embs[[paste('nn_',nn,'md_',md,'metric',metr,'1',sep = '.')]], 
            y = data_embs[[paste('nn_',nn,'md_',md,'metric',metr,'2',sep = '.')]]
            ))+
 geom_point(size = 0.3,alpha =0.1) +
 labs(x = "UMAP 1", y = "UMAP 2" ,
    title = paste('UMAP Main Components: ', paste('nn_',nn,'md_',md,'metric',metr,sep = '.')))+
  theme_bw() +
   ggsave(here('umap_trials/',paste(paste('nn_',nn,'md_',md,'metric',metr,sep = '.'),'.png')),
               height = 7, 
               width = 8)
  }
 }
 
}
```



#Applying GMM for clustering!

```{r}
# Modeling packages
library(mclust)   # for fitting clustering algorithms
# Apply GMM model with 3 components
#data_mc <- Mclust(umap_prep %>% juice() %>% select(nn_.50.md_.0.09.metric.cosine.1, nn_.50.md_.0.09.metric.cosine.2), G = 7)

# Plot results
#plot(data_mc, what = "density")
#plot(data_mc, what = "uncertainty")

data_optimal_mc <- Mclust(data_embs %>% select(nn_.50.md_.0.09.metric.cosine.1, nn_.50.md_.0.09.metric.cosine.2))
legend_args <- list(x = "bottomright", ncol = 5)
plot(data_optimal_mc, what = 'BIC', legendArgs = legend_args)
plot(data_optimal_mc, what = 'classification', legendArgs = list(x = "bottomright"))
plot(data_optimal_mc, what = 'uncertainty')


data_optimal_mc %>% mutate(cluster=classification)

umap_prep %>% juice() %>% mutate(cluster=data_optimal_mc$classification,
                                 uncertainty=data_optimal_mc$uncertainty)
```




#Testing

```{r}
#Visualising with the name of the clusters
if(require("mclust")){
# Compute model-based-clustering 
require("mclust")

# Visualize classification
fviz_mclust(data_optimal_mc, "classification", geom = "point")
}

data_embs %>% select(nn_.50.md_.0.09.metric.cosine.1, nn_.50.md_.0.09.metric.cosine.2)

components <- data_optimal_mc$data %>%  as_tibble()%>% mutate(clust=as.factor(data_optimal_mc$classification),
                                 uncertainty=data_optimal_mc$uncertainty)
library("viridis")
components %>% ggplot(aes(x=nn_.50.md_.0.09.metric.cosine.1, 
                          y=nn_.50.md_.0.09.metric.cosine.2, 
                          color=ifelse(uncertainty < 30, clust,'gray'))) +
  geom_point(alpha=0.3)+
  scale_color_viridis(discrete = FALSE, option = "C")+
  theme_minimal()

ggplot(aes(x=var1,y=var2, color=ifelse(column_1 < 30, col_groups,'grey')))






```


```{r}

data_cleaned_clusterized <- data_cleaned  #copy of data

data_cleaned_clusterized <- data_cleaned_clusterized %>% 
  mutate(clust = as.factor(data_optimal_mc$classification),
         uncertainty = data_optimal_mc$uncertainty)
  

# Density plots cluster
data_cleaned_clusterized %>% 
  ggplot(aes(x=monthly_stream30_followers, y=clust, fill=clust))+
  geom_joy(scale = 2, alpha=0.5) +
  scale_y_discrete(expand=c(0.01, 0)) +
  scale_x_continuous(limits = c(0, 300))+
  labs(y=NULL,
       x=NULL) +
  theme_minimal()+
  theme(legend.position="none")

# Checking if there is any statistical differences between clusters
# Monthly streams >30s followers
library(ggstatsplot)
data_cleaned_clusterized %>% filter(uncertainty<30) %>% 
ggbetweenstats(
  x = clust,
  y = monthly_stream30_followers,
  title = "Distribution of the monthly_stream30_followers across clusters "
) + scale_y_continuous(limits = c(0,35000))
  
#owner type

library(RColorBrewer)
mycolors <- colorRampPalette(brewer.pal(9, "lancet"))(nb.cols)
# Owner type
data_cleaned_clusterized %>% filter(uncertainty<30) %>% 
ggbarstats(
  x = owner_type,
  y = clust,
  title = "Owner type by Cluster",
  xlab = "cluster",
  legend.title = "Owner type",
  ggplot.component = list(ggplot2::scale_x_discrete(guide = ggplot2::guide_axis(n.dodge = 2))),
  palette = "Paired"
)


library(WRS2) # for data
library(afex) # to run anova



library(scales)

#mau_engagement
data_cleaned_clusterized %>% 
  filter(uncertainty<30) %>% 
  mutate(mau_engagement_perc=mau_engagement/100) %>% 
ggbetweenstats(
  x = clust,
  y = mau_engagement_perc,
  title = "Distribution of the MAU_engagement across clusters ",
  palette = "Paired" 
) + scale_y_continuous(labels = scales::percent)

#n_tracks
data_cleaned_clusterized %>% 
  filter(uncertainty<30) %>% 
  ggbetweenstats(
  x = clust,
  y = n_artists,
  title = "Distribution of the #Artist across clusters ",
  palette = "Paired" 
) + scale_y_continuous(limits = c(0,4500))



```

#Save important datasets

```{r}
write.csv(components,here('data','playlists_cleaned_mclust_optimal.csv'), row.names = FALSE)
write.csv(data_cleaned_clusterized,here('data','playlists_cleaned_clusterized.csv'), row.names = FALSE)


```


